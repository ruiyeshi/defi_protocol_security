# mine_exploits_combined.py  (v2 ‚Äì resilient + fallbacks)
"""
Unified exploit dataset miner (resilient version):
  - Rekt.News (HTML)
  - SlowMist (multiple DOM layouts)
  - Web3REKT (RSS + HTML fallback)
  - DeFiRanger (WaybackMachine snapshots)
  - De.Fi REKT API (primary) -> DefiLlama hacks (fallback)

Saves merged dataset -> exploits_combined.json
"""

from __future__ import annotations

import json
import time
import random
from pathlib import Path
from typing import List, Dict, Optional

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import feedparser
from datetime import datetime


OUTPUT_FILE = Path(__file__).parent / "exploits_combined.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Accept": "text/html,application/json;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.8",
    "Connection": "keep-alive",
}

def session_with_retries(total: int = 4, backoff: float = 0.6) -> requests.Session:
    s = requests.Session()
    retries = Retry(
        total=total,
        backoff_factor=backoff,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=frozenset(["GET", "HEAD"]),
        raise_on_status=False,
    )
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.mount("http://", HTTPAdapter(max_retries=retries))
    return s


def normalize_item(
    source: str,
    title: Optional[str],
    url: Optional[str],
    date: Optional[str],
    amount: Optional[float] = None,
    description: Optional[str] = None,
) -> Dict:
    # Normalize empty / whitespace titles
    t = (title or "").strip() or "Untitled"
    d = (description or "").strip() or None
    u = (url or "").strip()
    return {
        "source": source,
        "title": t,
        "url": u,
        "date": date,
        "amount": amount,
        "description": d,
    }


# -------------------------------
# 1. REKT.NEWS
# -------------------------------
def scrape_rekt() -> List[Dict]:
    print("üì∞ Scraping Rekt.News ...")
    base = "https://rekt.news"
    s = session_with_retries()
    try:
        r = s.get(base, headers=HEADERS, timeout=20)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        results: List[Dict] = []
        # Rekt index links are of the form "/project-name/"
        for a in soup.select("a[href^='/']"):
            href = a.get("href")
            if not href:
                continue
            # only single-segment paths like "/foo/"
            if href.count("/") == 1 and not href.startswith("/?"):
                url = base + href
                title = a.get_text(strip=True)
                if title:
                    results.append(normalize_item("Rekt.News", title, url, None))
        print(f"‚úÖ {len(results)} from Rekt.News")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è Rekt.News scraping failed: {e}")
        return []


# -------------------------------
# 2. SLOWMIST.IO (multiple DOMs)
# -------------------------------
def scrape_slowmist() -> List[Dict]:
    print("üõ°Ô∏è Scraping SlowMist ...")
    s = session_with_retries()
    url = "https://slowmist.io/en/security-advisories.html"
    backup_url = "https://www.slowmist.io/en/security-advisories.html"
    results: List[Dict] = []
    try:
        r = s.get(url, headers=HEADERS, timeout=25)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        # Layout A: cards
        cards = soup.select(".advisory-item")
        for card in cards:
            title_el = card.select_one(".advisory-title") or card.select_one("h3, h4, .title")
            date_el = card.select_one(".advisory-date")
            link_el = card.select_one("a")
            title = title_el.get_text(strip=True) if title_el else None
            date = date_el.get_text(strip=True) if date_el else None
            href = link_el.get("href") if link_el else None
            link = f"https://slowmist.io{href}" if href and href.startswith("/") else (href or "")
            results.append(normalize_item("SlowMist", title, link, date))

        # Layout B: table
        if not results:
            table = soup.select_one("table, .advisory-table")
            if table:
                for tr in table.select("tbody tr"):
                    cols = tr.find_all("td")
                    if not cols:
                        continue
                    # heuristic: last col link + first col date
                    date = cols[0].get_text(strip=True) if len(cols) >= 1 else None
                    link_el = tr.find("a")
                    href = link_el.get("href") if link_el else None
                    link = f"https://slowmist.io{href}" if href and href.startswith("/") else (href or "")
                    title = link_el.get_text(strip=True) if link_el else tr.get_text(strip=True)[:140]
                    results.append(normalize_item("SlowMist", title, link, date))

        print(f"‚úÖ {len(results)} from SlowMist")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è scrape_slowmist failed: {e}")
        return []


# -------------------------------
# 3. WEB3REKT (RSS + HTML fallback)
# -------------------------------
def scrape_web3rekt() -> List[Dict]:
    print("üíª Scraping Web3REKT ...")
    s = session_with_retries()
    results: List[Dict] = []

    # Primary RSS (Substack)
    rss_urls = [
        "https://web3rekt.substack.com/feed",
        "https://web3rekt.com/feed",
    ]
    for rss in rss_urls:
        try:
            resp = s.get(rss, headers=HEADERS, timeout=20)
            if resp.status_code >= 400 or not resp.content:
                continue
            feed = feedparser.parse(resp.content)
            if getattr(feed, "entries", None):
                for entry in feed.entries:
                    title = getattr(entry, "title", None)
                    link = getattr(entry, "link", "")
                    date = None
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        date_obj = datetime(*entry.published_parsed[:6])
                        date = date_obj.strftime("%Y-%m-%d")
                    elif hasattr(entry, "published"):
                        date = entry.published
                    desc = getattr(entry, "summary", None)
                    results.append(normalize_item("Web3REKT", title, link, date, description=desc))
                break
        except Exception:
            continue

    # HTML fallback if RSS empty
    if not results:
        try:
            page = "https://web3rekt.substack.com"
            r = s.get(page, headers=HEADERS, timeout=20)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.select("a[href*='/p/']"):
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = href if href.startswith("http") else page.rstrip("/") + href
                if title:
                    results.append(normalize_item("Web3REKT", title, link, None))
        except Exception as e:
            print(f"‚ö†Ô∏è Web3REKT fallback failed: {e}")

    print(f"‚úÖ {len(results)} from Web3REKT")
    return results


# -------------------------------
# 4. DEFI.RANGER via Wayback
# -------------------------------
WAYBACK_CANDIDATES = [
    "https://web.archive.org/web/20230901000000/https://defiranger.io/incidents",
    "https://web.archive.org/web/20230701000000/https://defiranger.io/incidents",
    "https://web.archive.org/web/20230501000000/https://defiranger.io/incidents",
    "https://web.archive.org/web/20240501000000/https://defiranger.io/incidents",
    "https://web.archive.org/web/20230901000000/https://defiranger.io/incidents",
]

def scrape_defiranger() -> List[Dict]:
    print("üîç Scraping DeFiRanger (Wayback) ...")
    s = session_with_retries()
    results: List[Dict] = []
    for url in WAYBACK_CANDIDATES:
        try:
            r = s.get(url, headers=HEADERS, timeout=25)
            if r.status_code >= 400 or not r.text:
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            # Look for cards or list items
            cards = soup.select("div.card, .incident-card")
            if not cards:
                items = soup.select("li a[href]")
                for a in items:
                    title = a.get_text(strip=True)
                    href = a.get("href", "")
                    link = href if href.startswith("http") else "https://defiranger.io" + href
                    if title:
                        results.append(normalize_item("DeFiRanger", title, link, None))
            else:
                for div in cards:
                    title_el = div.find(["h3", "h4", "h5"])
                    title = title_el.get_text(strip=True) if title_el else None
                    link_el = div.find("a")
                    href = link_el.get("href") if link_el else ""
                    link = href if href.startswith("http") else "https://defiranger.io" + href
                    date_el = div.select_one(".date, .time, time")
                    date = date_el.get_text(strip=True) if date_el else None
                    results.append(normalize_item("DeFiRanger", title, link, date))
            if results:
                break
        except Exception:
            continue

    print(f"‚úÖ {len(results)} from DeFiRanger (archived)")
    return results


# -------------------------------
# 5. DE.FI API (primary) -> DefiLlama (fallback)
# -------------------------------
def scrape_defi_api() -> List[Dict]:
    print("üåê Scraping De.Fi API (with DefiLlama fallback) ...")
    s = session_with_retries()
    primary = "https://api.de.fi/rekt"
    fallback = "https://api.defillama.com/v1/hacks"
    results: List[Dict] = []

    try:
        res = s.get(primary, headers={**HEADERS, "Accept": "application/json"}, timeout=25)
        res.raise_for_status()
        try:
            data = res.json()
        except Exception:
            data = None
        if isinstance(data, list):
            for item in data:
                results.append(
                    normalize_item(
                        "De.Fi",
                        item.get("project") or item.get("name"),
                        item.get("source") or item.get("url") or "",
                        item.get("date"),
                        item.get("amountUsd") or item.get("amount"),
                        item.get("description"),
                    )
                )
            print(f"‚úÖ {len(results)} from De.Fi API")
            return results
        else:
            raise ValueError("Empty/invalid JSON from De.Fi")
    except Exception as e:
        print(f"‚ö†Ô∏è De.Fi API primary failed: {e}. Trying DefiLlama fallback...")

    # Fallback ‚Äì DefiLlama
    try:
        res = s.get(fallback, headers={**HEADERS, "Accept": "application/json"}, timeout=25)
        res.raise_for_status()
        data = res.json()
        if isinstance(data, list):
            for item in data:
                results.append(
                    normalize_item(
                        "De.Fi (DefiLlama)",
                        item.get("name"),
                        item.get("link") or "",
                        item.get("date"),
                        item.get("amount"),
                        item.get("description"),
                    )
                )
            print(f"‚úÖ {len(results)} from DefiLlama fallback")
            return results
    except Exception as e2:
        print(f"‚ö†Ô∏è DefiLlama fallback failed: {e2}")

    return results


# -------------------------------
# MAIN
# -------------------------------
def main():
    all_data: List[Dict] = []
    for func in [scrape_rekt, scrape_slowmist, scrape_web3rekt, scrape_defiranger, scrape_defi_api]:
        try:
            res = func()
            all_data.extend(res)
            time.sleep(random.uniform(0.8, 1.6))
        except Exception as e:
            print(f"‚ö†Ô∏è {func.__name__} failed: {e}")

    print(f"\nüíæ Total collected: {len(all_data)} incidents")
    try:
        OUTPUT_FILE.write_text(json.dumps(all_data, indent=2, ensure_ascii=False))
        print(f"üìÅ Saved to {OUTPUT_FILE}")
    except Exception as e:
        print(f"‚ùå Failed to write output: {e}")


if __name__ == "__main__":
    main()