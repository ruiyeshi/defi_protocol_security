#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified DeFi exploit miner (2016‚Äìpresent) with retries, fallbacks and caching.

Sources implemented:
- Rekt.News (site pages)
- SlowMist (new hack feed)
- Web3REKT (Substack archive + page fetch; graceful if blocked)
- DeFiRanger (Wayback fallback)
- De.Fi API and HTML fallback
- DefiLlama hacks API and HTML fallback

Output:
  data_raw/exploits_raw/exploits_combined_unified.json
"""

import json, re, time, random
from pathlib import Path
from typing import List, Dict, Any, Optional
import requests
from bs4 import BeautifulSoup

import logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

print("üöÄ Starting unified exploit miner script... (sanity check)")

# -------------------- Paths & constants --------------------
ROOT = Path(__file__).resolve().parents[2]  # project root
OUT_DIR = ROOT / "data_raw" / "exploits_raw"
OUT_DIR.mkdir(parents=True, exist_ok=True)
OUT_JSON = OUT_DIR / "exploits_combined_unified.json"
CACHE_DIR = OUT_DIR / "_cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

UA = "Mozilla/5.0 (compatible; DeFiResearchBot/1.0; +https://github.com/ruiyeshi)"
HEADERS = {"User-Agent": UA, "Accept": "text/html,application/json;q=0.9,*/*;q=0.8"}
TIMEOUT = 20
RETRIES = 3
SLEEP_RANGE = (0.8, 1.8)

# -------------------- Helpers --------------------
def sleep_jitter(a=0.5, b=1.5):
    time.sleep(random.uniform(a, b))

def get_session() -> requests.Session:
    s = requests.Session()
    s.headers.update(HEADERS)
    return s

def fetch_json(url: str, session: requests.Session, *, cache_key: Optional[str]=None) -> Optional[dict]:
    if cache_key:
        cp = CACHE_DIR / f"{cache_key}.json"
        if cp.exists():
            try:
                return json.loads(cp.read_text())
            except Exception:
                pass
    for i in range(RETRIES):
        try:
            r = session.get(url, timeout=TIMEOUT)
            r.raise_for_status()
            data = r.json()
            if cache_key:
                (CACHE_DIR / f"{cache_key}.json").write_text(json.dumps(data))
            return data
        except Exception:
            sleep_jitter(*SLEEP_RANGE)
    return None

def fetch_text(url: str, session: requests.Session, *, cache_key: Optional[str]=None) -> Optional[str]:
    if cache_key:
        cp = CACHE_DIR / f"{cache_key}.html"
        if cp.exists():
            try:
                return cp.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                pass
    for i in range(RETRIES):
        try:
            r = session.get(url, timeout=TIMEOUT)
            r.raise_for_status()
            txt = r.text
            if cache_key:
                (CACHE_DIR / f"{cache_key}.html").write_text(txt)
            return txt
        except Exception:
            sleep_jitter(*SLEEP_RANGE)
    return None

def clean_text(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"\s+", " ", s).strip()

def parse_usd(s: str) -> Optional[float]:
    if not s:
        return None
    m = re.search(r"(\$|USD)?\s?([0-9][0-9,\.]+)\s*(million|billion|m|bn)?", s, re.I)
    if not m:
        return None
    val = m.group(2).replace(",", "")
    try:
        x = float(val)
        suffix = (m.group(3) or "").lower()
        if suffix in ("million", "m"):
            x *= 1_000_000
        elif suffix in ("billion", "bn"):
            x *= 1_000_000_000
        return x
    except:
        return None

def norm_record(source, title, url, date="", text="", protocol=None, loss_usd=None,
                exploit_type=None, contract_address=None, chain=None) -> Dict[str, Any]:
    return {
        "source": source,
        "title": clean_text(title or "Untitled"),
        "url": url,
        "date": (date or "").strip(),
        "text": clean_text(text),
        "protocol": protocol or "",
        "loss_usd": loss_usd,
        "exploit_type": exploit_type or "",
        "contract_address": (contract_address or "").lower(),
        "chain": chain or ""
    }

def dedupe(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    out = []
    for r in rows:
        key = (r.get("title",""), r.get("url",""), r.get("date",""))
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out

# -------------------- Sources --------------------
def scrape_rekt(session: requests.Session) -> List[Dict[str, Any]]:
    print("üì∞ Rekt.News ‚Ä¶")
    results = []
    base = "https://rekt.news"
    idx = fetch_text(base, session, cache_key="rekt_index")
    if not idx:
        print("  ‚ö†Ô∏è index fetch failed")
        logging.warning("Rekt.News index fetch failed")
        return results
    soup = BeautifulSoup(idx, "html.parser")
    # candidate links:
    links = [a["href"] for a in soup.select("a[href]") if "/rekt" in a.get("href","") or "/rugged" in a.get("href","")]
    links = list(dict.fromkeys(links))[:120]  # limit
    for i, href in enumerate(links):
        url = href if href.startswith("http") else f"{base}{href}"
        html = fetch_text(url, session, cache_key=f"rekt_{i}")
        if not html:
            continue
        s = BeautifulSoup(html, "html.parser")
        title = s.find("h1").get_text(strip=True) if s.find("h1") else ""
        date = (s.find("time") or s.find("span", class_="date"))
        date = date.get_text(strip=True) if date else ""
        paras = " ".join([p.get_text(" ", strip=True) for p in s.find_all("p")])
        # cheap hints
        loss = None
        if "loss" in paras.lower() or "$" in paras:
            loss = parse_usd(paras)
        results.append(norm_record("Rekt.News", title, url, date, paras))
        sleep_jitter(*SLEEP_RANGE)
    print(f"  ‚úÖ {len(results)} from Rekt.News")
    return results

def scrape_slowmist(session: requests.Session) -> List[Dict[str, Any]]:
    print("üõ°Ô∏è SlowMist hack feed ‚Ä¶")
    results = []
    url = "https://hacked.slowmist.io/en/"
    html = fetch_text(url, session, cache_key="slowmist_hackfeed")
    if not html:
        print("  ‚ö†Ô∏è SlowMist hack feed fetch failed")
        logging.warning("SlowMist hack feed fetch failed")
        return results
    soup = BeautifulSoup(html, "html.parser")
    articles = soup.find_all("article")
    if not articles:
        print("  ‚ö†Ô∏è No articles found in SlowMist hack feed")
        logging.warning("No articles found in SlowMist hack feed")
        return results
    for i, art in enumerate(articles):
        # Extract title and link
        a_tag = art.find("a")
        title = a_tag.get_text(strip=True) if a_tag else ""
        link = a_tag["href"] if a_tag and a_tag.has_attr("href") else ""
        if link and not link.startswith("http"):
            link = "https://hacked.slowmist.io" + link
        # Extract date
        date_tag = art.find("time")
        date = date_tag.get_text(strip=True) if date_tag else ""
        # Extract short description
        desc_tag = art.find("p")
        desc = desc_tag.get_text(" ", strip=True) if desc_tag else ""
        results.append(norm_record("SlowMist", title, link, date, desc))
        sleep_jitter(*SLEEP_RANGE)
    print(f"  ‚úÖ {len(results)} from SlowMist hack feed")
    return results

def scrape_web3rekt(session: requests.Session) -> List[Dict[str, Any]]:
    print("üíª Web3REKT (Substack) ‚Ä¶")
    # Substack archive may block requests; try archive page, then fall back to no-op gracefully
    base = "https://web3rekt.substack.com/archive"
    html = fetch_text(base, session, cache_key="web3rekt_archive")
    if not html:
        print("  ‚ö†Ô∏è archive not reachable, skipping")
        logging.warning("Web3REKT archive not reachable")
        return []
    soup = BeautifulSoup(html, "html.parser")
    links = [a["href"] for a in soup.select("a[href*='/p/']")]
    links = list(dict.fromkeys(links))[:50]
    out = []
    for i, url in enumerate(links):
        page = fetch_text(url, session, cache_key=f"web3rekt_{i}")
        if not page:
            continue
        s = BeautifulSoup(page, "html.parser")
        title = s.find("h1").get_text(strip=True) if s.find("h1") else ""
        date_el = s.find("time")
        date = date_el.get_text(strip=True) if date_el else ""
        text = " ".join([p.get_text(" ", strip=True) for p in s.find_all("p")])
        out.append(norm_record("Web3REKT", title, url, date, text))
        sleep_jitter(*SLEEP_RANGE)
    print(f"  ‚úÖ {len(out)} from Web3REKT")
    return out

def scrape_defiranger(session: requests.Session) -> List[Dict[str, Any]]:
    print("üîç DeFiRanger (Wayback fallback) ‚Ä¶")
    # live API (often down), then Wayback JSON (if exists)
    out = []
    live = fetch_json("https://defiranger.io/incidents", session, cache_key="defiranger_live")
    data = None
    if isinstance(live, dict) and "incidents" in live:
        data = live
    else:
        # try archived JSON if any (example snapshot path; may or may not exist)
        wb = fetch_text(
            "https://web.archive.org/web/20240401/https://defiranger.io/incidents",
            session,
            cache_key="defiranger_wb"
        )
        try:
            data = json.loads(wb) if wb else None
        except Exception:
            data = None

    if isinstance(data, dict) and data.get("incidents"):
        for x in data["incidents"]:
            out.append(norm_record(
                "DeFiRanger",
                x.get("name",""),
                x.get("url",""),
                x.get("date",""),
                x.get("description",""),
                protocol=x.get("protocol"),
                loss_usd=x.get("loss_usd"),
                exploit_type=x.get("type"),
                contract_address=x.get("contract"),
                chain=x.get("chain"),
            ))
    print(f"  ‚úÖ {len(out)} from DeFiRanger")
    return out

def scrape_defi_api_or_defillama(session: requests.Session) -> List[Dict[str, Any]]:
    print("üåê De.Fi API and HTML fallback, DefiLlama API and HTML fallback ‚Ä¶")
    out = []
    # De.Fi API
    defi = fetch_json("https://api.de.fi/rekt", session, cache_key="defi_rekt")
    if isinstance(defi, dict) and defi.get("data"):
        for x in defi["data"]:
            out.append(norm_record(
                "De.Fi",
                x.get("title",""),
                x.get("link",""),
                x.get("date",""),
                x.get("description",""),
                protocol=x.get("protocol"),
                loss_usd=x.get("lossUsd") or x.get("loss_usd"),
                exploit_type=x.get("type"),
                contract_address=x.get("contract"),
                chain=x.get("chain"),
            ))
        print(f"  ‚úÖ {len(out)} from De.Fi API")
        return out
    else:
        print("  ‚ö†Ô∏è De.Fi API unavailable, trying HTML fallback")
        html = fetch_text("https://de.fi/rekt-database", session, cache_key="defi_rekt_html")
        if html:
            soup = BeautifulSoup(html, "html.parser")
            rows = soup.select("div.table-row")
            for r in rows:
                title = r.select_one("div.title")
                title_text = title.get_text(strip=True) if title else ""
                url_tag = r.select_one("a")
                url = url_tag["href"] if url_tag and url_tag.has_attr("href") else ""
                date_tag = r.select_one("div.date")
                date = date_tag.get_text(strip=True) if date_tag else ""
                desc_tag = r.select_one("div.description")
                desc = desc_tag.get_text(" ", strip=True) if desc_tag else ""
                protocol_tag = r.select_one("div.protocol")
                protocol = protocol_tag.get_text(strip=True) if protocol_tag else ""
                loss_tag = r.select_one("div.loss")
                loss_usd = parse_usd(loss_tag.get_text(strip=True)) if loss_tag else None
                exploit_type_tag = r.select_one("div.type")
                exploit_type = exploit_type_tag.get_text(strip=True) if exploit_type_tag else ""
                contract_tag = r.select_one("div.contract")
                contract_address = contract_tag.get_text(strip=True) if contract_tag else ""
                chain_tag = r.select_one("div.chain")
                chain = chain_tag.get_text(strip=True) if chain_tag else ""
                out.append(norm_record(
                    "De.Fi HTML",
                    title_text,
                    url,
                    date,
                    desc,
                    protocol=protocol,
                    loss_usd=loss_usd,
                    exploit_type=exploit_type,
                    contract_address=contract_address,
                    chain=chain
                ))
            print(f"  ‚úÖ {len(out)} from De.Fi HTML fallback")
            return out
        else:
            print("  ‚ö†Ô∏è De.Fi HTML fallback fetch failed")
            logging.warning("De.Fi HTML fallback fetch failed")

    # DefiLlama API
    hacks = fetch_json("https://api.llama.fi/hacks", session, cache_key="llama_hacks")
    if isinstance(hacks, dict) and hacks.get("hacks"):
        for x in hacks["hacks"]:
            out.append(norm_record(
                "DefiLlama",
                x.get("name",""),
                x.get("link",""),
                x.get("date",""),
                x.get("description",""),
                protocol=x.get("protocol"),
                loss_usd=x.get("lossUsd") or x.get("loss_usd"),
                exploit_type=" / ".join(x.get("exploits", [])) if isinstance(x.get("exploits"), list) else x.get("type") or "",
                contract_address=(x.get("addresses") or [None])[0] if isinstance(x.get("addresses"), list) else x.get("address"),
                chain=(x.get("chains") or [None])[0] if isinstance(x.get("chains"), list) else x.get("chain"),
            ))
        print(f"  ‚úÖ {len(out)} from DefiLlama API")
        return out
    else:
        print("  ‚ö†Ô∏è DefiLlama API unavailable, trying HTML fallback")
        html = fetch_text("https://defillama.com/hacks", session, cache_key="defillama_hacks_html")
        if html:
            soup = BeautifulSoup(html, "html.parser")
            # Assuming each hack is in a container with class 'hack-card' or similar
            cards = soup.select("div.hack-card, div.card, div.hack")
            for c in cards:
                title_tag = c.select_one("h3, h2, a")
                title = title_tag.get_text(strip=True) if title_tag else ""
                link_tag = c.select_one("a")
                link = link_tag["href"] if link_tag and link_tag.has_attr("href") else ""
                if link and not link.startswith("http"):
                    link = "https://defillama.com" + link
                date_tag = c.select_one(".date, .hack-date")
                date = date_tag.get_text(strip=True) if date_tag else ""
                desc_tag = c.select_one(".description, .desc, p")
                desc = desc_tag.get_text(" ", strip=True) if desc_tag else ""
                protocol = ""
                loss_usd = None
                exploit_type = ""
                contract_address = ""
                chain = ""
                # Try to extract protocol, loss, type, contract, chain from text if possible
                text_all = c.get_text(" ", strip=True)
                loss_usd = parse_usd(text_all)
                out.append(norm_record(
                    "DefiLlama HTML",
                    title,
                    link,
                    date,
                    desc,
                    protocol=protocol,
                    loss_usd=loss_usd,
                    exploit_type=exploit_type,
                    contract_address=contract_address,
                    chain=chain
                ))
            print(f"  ‚úÖ {len(out)} from DefiLlama HTML fallback")
            return out
        else:
            print("  ‚ö†Ô∏è DefiLlama HTML fallback fetch failed")
            logging.warning("DefiLlama HTML fallback fetch failed")

    print("  ‚ö†Ô∏è Both De.Fi and DefiLlama sources unavailable")
    return out

# -------------------- Orchestrator --------------------
def main():
    print("üöÄ Starting unified exploit miner...")
    logging.info("Starting unified exploit miner (multi-source)...")
    logging.info("Initializing unified scraper...")
    session = get_session()
    all_rows = []
    logging.info("Fetching incidents from all sources sequentially...")

    for fn in [scrape_rekt, scrape_slowmist, scrape_web3rekt, scrape_defiranger, scrape_defi_api_or_defillama]:
        try:
            rows = fn(session)
            all_rows.extend(rows)
        except Exception as e:
            logging.warning(f"{fn.__name__} crashed: {e}")

    all_rows = dedupe(all_rows)
    logging.info(f"Total incidents collected: {len(all_rows)}")

    OUT_JSON.write_text(json.dumps(all_rows, indent=2, ensure_ascii=False))
    logging.info(f"Saved to {OUT_JSON}")
    logging.info(f"‚úÖ Done. Unified dataset saved to {OUT_JSON} with {len(all_rows)} total incidents.")

if __name__ == "__main__":
    main()
    print("‚úÖ Script finished running successfully.")