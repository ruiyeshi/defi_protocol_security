import requests
import logging
import time
import os
import csv
import subprocess
import json

# Configure logging
logging.basicConfig(filename='logs/pipeline_run.log', level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')

def scrape_slowmist():
    logging.info("Starting scrape of SlowMist API")
    results = []
    page = 1
    base_url = "https://hacked.slowmist.io/api/hack?lang=en&page={page}&limit=100"

    while True:
        url = base_url.format(page=page)
        attempts = 0
        success = False
        while attempts < 3 and not success:
            try:
                response = requests.get(url, timeout=15)
                response.raise_for_status()
                data = response.json()
                success = True
            except Exception as e:
                attempts += 1
                time.sleep(2)
        if not success:
            logging.warning(f"Failed to fetch SlowMist API page {page} after 3 attempts.")
            break

        hack_list = data.get("list", [])
        if not hack_list:
            break

        for item in hack_list:
            title = item.get("title")
            amount = item.get("amount")
            attack_method = item.get("attackMethod")
            description = item.get("desc")
            date = item.get("date")
            ecosystem = item.get("ecosystem") if "ecosystem" in item else None
            id_ = item.get("id")

            results.append({
                "source": "SlowMist",
                "title": title,
                "date": date,
                "loss_usd": amount,
                "attack_method": attack_method,
                "description": description,
                "ecosystem": ecosystem,
                "url": f"https://hacked.slowmist.io/#/detail/{id_}"
            })

        logging.info(f"Collected {len(results)} entries so far from SlowMist API")
        page += 1

    logging.info(f"âœ… {len(results)} from SlowMist API")
    return results


def run_poc_analysis():
    logging.info("Starting POC analysis with Slither and Mythril")
    poc_dir = "data_raw/contracts/poc_samples/cross-contract-reentrancy-poc"
    os.makedirs(poc_dir, exist_ok=True)

    # List all solidity files in the POC directory
    poc_files = []
    for root, dirs, files in os.walk(poc_dir):
        for file in files:
            if file.endswith(".sol"):
                poc_files.append(os.path.join(root, file))

    results = []
    for sol_file in poc_files:
        file_name = os.path.relpath(sol_file, poc_dir)
        # Run Slither analysis
        try:
            slither_cmd = ["slither", sol_file, "--json", "slither_output.json"]
            subprocess.run(slither_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            with open("slither_output.json") as f:
                slither_data = json.load(f)
            # Parse Slither results for vulnerabilities
            for issue in slither_data.get("results", {}).get("detectors", []):
                vuln_type = issue.get("check")
                severity = issue.get("severity")
                results.append({
                    "file_name": file_name,
                    "tool": "Slither",
                    "vulnerability_type": vuln_type,
                    "severity": severity,
                    "detected": 1
                })
        except Exception as e:
            logging.warning(f"Slither analysis failed for {file_name}: {e}")

        # Run Mythril analysis
        try:
            mythril_cmd = ["myth", "analyze", sol_file, "--json", "-o", "mythril_output.json"]
            subprocess.run(mythril_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            with open("mythril_output.json") as f:
                mythril_data = json.load(f)
            for issue in mythril_data.get("issues", []):
                vuln_type = issue.get("title")
                severity = issue.get("severity")
                results.append({
                    "file_name": file_name,
                    "tool": "Mythril",
                    "vulnerability_type": vuln_type,
                    "severity": severity,
                    "detected": 1
                })
        except Exception as e:
            logging.warning(f"Mythril analysis failed for {file_name}: {e}")

    # Write results to CSV
    csv_path = "data_processed/poc_analysis_results.csv"
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    with open(csv_path, "w", newline="") as csvfile:
        fieldnames = ["file_name", "tool", "vulnerability_type", "severity", "detected"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in results:
            writer.writerow(row)

    logging.info(f"POC analysis completed with {len(results)} total findings")
    return results


def append_poc_to_main_dataset(main_dataset):
    logging.info("Appending POC validation results to main dataset")
    csv_path = "data_processed/poc_analysis_results.csv"
    if not os.path.exists(csv_path):
        logging.warning("POC analysis results CSV not found, skipping append")
        return main_dataset

    poc_results = []
    with open(csv_path, newline="") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            poc_results.append(row)

    # Append a new block labelled source: POC_validation
    main_dataset.append({
        "source": "POC_validation",
        "data": poc_results
    })

    logging.info(f"Appended {len(poc_results)} POC validation entries to main dataset")
    return main_dataset


def enrich_main_dataset_with_poc(main_dataset, poc_results):
    logging.info("Enriching main dataset with POC similarity scores and flags")
    # Build sets of vulnerability types from POC results by tool
    slither_vulns = set()
    mythril_vulns = set()
    for entry in poc_results:
        if entry["tool"] == "Slither":
            slither_vulns.add(entry["vulnerability_type"])
        elif entry["tool"] == "Mythril":
            mythril_vulns.add(entry["vulnerability_type"])

    # For each contract in main dataset, add is_poc_like_pattern and poc_similarity_score
    for contract in main_dataset:
        # Example: check if contract's known vulnerabilities overlap with POC detected vulnerabilities
        contract_vulns = set()
        # Assuming contract has a field 'vulnerabilities' listing types detected by Slither/Mythril
        if "vulnerabilities" in contract:
            contract_vulns = set(contract["vulnerabilities"])
        else:
            contract_vulns = set()

        overlap_slither = len(contract_vulns.intersection(slither_vulns))
        overlap_mythril = len(contract_vulns.intersection(mythril_vulns))
        total_poc_vulns = len(slither_vulns) + len(mythril_vulns)
        if total_poc_vulns == 0:
            similarity_score = 0.0
        else:
            similarity_score = (overlap_slither + overlap_mythril) / total_poc_vulns

        contract["is_poc_like_pattern"] = 1 if similarity_score > 0 else 0
        contract["poc_similarity_score"] = round(similarity_score, 3)

    logging.info("Main dataset enrichment with POC patterns complete")
    return main_dataset