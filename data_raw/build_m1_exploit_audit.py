# data_raw/build_m1_exploit_audit.py
import argparse
import os
import re
from datetime import datetime

import numpy as np
import pandas as pd


def norm_name(x: str) -> str:
    """Normalize protocol names for matching."""
    if pd.isna(x):
        return ""
    s = str(x).lower().strip()
    s = s.replace("&", "and")
    s = re.sub(r"[^a-z0-9]+", " ", s)
    s = " ".join(s.split())
    return s


def parse_dt_series(s: pd.Series) -> pd.Series:
    """
    Robust datetime parsing:
    - if looks numeric and extremely small/large, try seconds/ms
    - else parse as string datetime
    """
    dt = pd.to_datetime(s.astype(str), utc=True, errors="coerce")

    # If most are NaT, try numeric epoch heuristics
    if dt.notna().mean() < 0.5:
        sn = pd.to_numeric(s, errors="coerce")
        med = sn.dropna().median() if sn.notna().any() else None
        if med is not None:
            unit = "ms" if med > 1e11 else "s"
            dt2 = pd.to_datetime(sn, unit=unit, utc=True, errors="coerce")
            if dt2.notna().mean() > dt.notna().mean():
                dt = dt2
    return dt


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--exploits", required=True, help="Path to exploit_events_defillama.csv")
    ap.add_argument("--audits", required=True, help="Path to audit_master_with_slug_defi_only.csv")
    ap.add_argument("--out", required=True, help="Output path for m1.csv")
    args = ap.parse_args()

    ex = pd.read_csv(args.exploits)
    au = pd.read_csv(args.audits)

    # --- Exploits: normalize & parse dates
    if "protocol_name_raw" not in ex.columns:
        raise ValueError("Expected column protocol_name_raw in exploits file.")
    ex["proto_norm"] = ex["protocol_name_raw"].map(norm_name)

    ex["exploit_dt"] = parse_dt_series(ex.get("exploit_date", pd.Series([None] * len(ex))))
    ex["exploit_day"] = ex["exploit_dt"].dt.floor("D")

    # --- Audits: normalize & parse last audit date
    if "proto_norm" not in au.columns and "protocol_name_raw" in au.columns:
        au["proto_norm"] = au["protocol_name_raw"].map(norm_name)

    au["proto_norm2"] = au["proto_norm"].map(norm_name) if "proto_norm" in au.columns else ""

    au["last_audit_date_dt"] = parse_dt_series(au.get("last_audit_date", pd.Series([None] * len(au))))

    # Keep only columns we actually want from audits (avoid enormous merge bloat)
    keep_cols = [
        "protocol_name_raw", "proto_norm2", "slug", "name",
        "has_audit", "audit_firm_count", "audit_score",
        "any_top_firm", "last_audit_date_dt",
        "audit_sources", "audit_firms",
        "category_llama", "tvl", "chains_llama"
    ]
    keep_cols = [c for c in keep_cols if c in au.columns]
    au_small = au[keep_cols].copy()

    # --- Merge: more robust matching (proto_norm, audit name, slug)
    # Create a stable row id so we can deduplicate after joining on multiple keys
    ex = ex.copy()
    ex["_row_id"] = np.arange(len(ex), dtype=int)

    # Build a key table from audits. Priority order: proto_norm2 > name > slug.
    au_work = au_small.copy()
    key_frames = []

    def _make_key_frame(col_name: str, key_type: str) -> pd.DataFrame:
        k = au_work[[col_name]].copy()
        k["key_norm"] = k[col_name].astype(str).map(norm_name)
        k["key_type"] = key_type
        # drop the original key col to avoid duplicate columns on join
        rest = au_work.drop(columns=[col_name], errors="ignore")
        return k[["key_norm", "key_type"]].join(rest)

    if "proto_norm2" in au_work.columns:
        key_frames.append(_make_key_frame("proto_norm2", "proto_norm2"))
    if "name" in au_work.columns:
        key_frames.append(_make_key_frame("name", "name"))
    if "slug" in au_work.columns:
        key_frames.append(_make_key_frame("slug", "slug"))

    au_keys = pd.concat(key_frames, ignore_index=True) if key_frames else pd.DataFrame(columns=["key_norm", "key_type"])
    au_keys = au_keys[au_keys["key_norm"].astype(str).str.len() > 0].copy()

    # If multiple audit rows share a key, keep the one with the strongest signals.
    for c in ["has_audit", "any_top_firm", "audit_score", "audit_firm_count"]:
        if c in au_keys.columns:
            au_keys[c] = pd.to_numeric(au_keys[c], errors="coerce").fillna(0)

    sort_cols = [c for c in ["has_audit", "any_top_firm", "audit_score", "audit_firm_count"] if c in au_keys.columns]
    if sort_cols:
        au_keys = au_keys.sort_values(sort_cols, ascending=False)

    au_keys = au_keys.drop_duplicates(subset=["key_norm", "key_type"], keep="first")

    # Join exploits to the key table on proto_norm
    m1_raw = ex.merge(
        au_keys,
        left_on="proto_norm",
        right_on="key_norm",
        how="left",
        indicator=True
    )

    # If multiple key types match, keep best key_type per exploit row
    key_priority = {"proto_norm2": 0, "name": 1, "slug": 2}
    m1_raw["_key_priority"] = m1_raw["key_type"].map(key_priority).fillna(99).astype(int)

    m1 = (
        m1_raw.sort_values(["_row_id", "_key_priority"])
              .drop_duplicates(subset=["_row_id"], keep="first")
              .drop(columns=["_key_priority"], errors="ignore")
    )

    # --- Derived variables for analysis
    m1["has_audit_record"] = pd.to_numeric(m1.get("has_audit"), errors="coerce").fillna(0).astype(int)

    # --- Robust audit indicators (data quality fix)
    audit_score_num = pd.to_numeric(m1.get("audit_score"), errors="coerce").fillna(0)
    audit_firm_count_num = pd.to_numeric(m1.get("audit_firm_count"), errors="coerce").fillna(0)

    sources_str = m1.get("audit_sources", "").astype(str).str.lower()
    firms_str = m1.get("audit_firms", "").astype(str).str.lower()

    # (A) Full-audit style indicator: score>0 OR firm_count>0 OR explicit 'full' tag
    m1["has_full_audit"] = (
        (audit_score_num > 0)
        | (audit_firm_count_num > 0)
        | (sources_str.str.contains("full", na=False))
    ).astype(int)

    # (B) CertiK badge indicator: 'certik' appears in sources or firms
    m1["has_certik_badge"] = (
        sources_str.str.contains("certik", na=False)
        | firms_str.str.contains("certik", na=False)
    ).astype(int)

    m1["audit_exists_before_exploit"] = (
        (m1["has_audit_record"] == 1)
        & (m1["last_audit_date_dt"].notna())
        & (m1["exploit_dt"].notna())
        & (m1["last_audit_date_dt"] <= m1["exploit_dt"])
    ).astype(int)

    m1["days_since_last_audit"] = (m1["exploit_dt"] - m1["last_audit_date_dt"]).dt.days

    # --- QA prints
    total = len(m1)
    matched = (m1["_merge"] == "both").sum()
    print(f"Rows: {total}")
    print(f"Matched to audits: {matched} ({matched/total:.1%})")

    if "key_type" in m1.columns:
        print("Match key_type breakdown:")
        print(m1["key_type"].fillna("(unmatched)").value_counts().to_string())

    print(f"has_full_audit rate (rows): {m1['has_full_audit'].mean():.3%}")
    print(f"has_certik_badge rate (rows): {m1['has_certik_badge'].mean():.3%}")

    print("Top 10 unmatched by loss_usd:")
    if "loss_usd" in m1.columns:
        cols_show = [c for c in [
            "protocol_name_raw", "loss_usd", "chain", "exploit_type", "exploit_dt",
            "proto_norm", "key_type", "slug", "name"
        ] if c in m1.columns]
        print(m1.loc[m1["_merge"] != "both"].sort_values("loss_usd", ascending=False)[cols_show].head(10).to_string(index=False))

    # --- Save
    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    m1 = m1.drop(columns=["_row_id"], errors="ignore")
    m1.to_csv(args.out, index=False)
    print("Saved:", args.out)


if __name__ == "__main__":
    main()