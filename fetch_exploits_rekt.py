import requests
import pandas as pd
from bs4 import BeautifulSoup
from difflib import SequenceMatcher
import time
import re

# Load verified protocols
verified_path = "data_raw/contracts/verified_with_controls.csv"
verified = pd.read_csv(verified_path)
protocols = [p.lower() for p in verified["protocol_name"].dropna().tolist()]

exploit_data = []
visited = set()

def similar(a, b):
    """Check fuzzy match between two strings."""
    return SequenceMatcher(None, a, b).ratio() > 0.7

def should_skip(url):
    """Avoid crawling tag, leaderboard, and paginated URLs."""
    bad_patterns = ["tag=", "leaderboard", "page=", "videos", "account", "solscan"]
    return any(bp in url for bp in bad_patterns)

def get_links(url, depth=1, max_depth=2):
    """Collect limited internal links recursively."""
    if depth > max_depth or url in visited or should_skip(url):
        return []
    visited.add(url)
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True)]
        full_links = []
        for l in links:
            if l.startswith("http") and "rekt.news" in l:
                full_links.append(l)
            elif l.startswith("/"):
                full_links.append("https://rekt.news" + l)
        # Remove duplicates & bad ones
        full_links = [l for l in set(full_links) if not should_skip(l)]
        return full_links
    except Exception as e:
        print(f"âš ï¸ Skipping {url}: {e}")
        return []

# Start from homepage
base = "https://rekt.news"
to_visit = get_links(base, 1)

for link in to_visit:
    if should_skip(link):
        continue
    print(f"ðŸ” Scanning: {link}")
    try:
        page = requests.get(link, timeout=10).text.lower()
        for name in protocols:
            if name in page or any(similar(name, word) for word in page.split()):
                exploit_data.append({
                    "protocol_name": name,
                    "source": "Rekt",
                    "exploit_link": link
                })
                print(f"ðŸ’¥ Found exploit match: {name} -> {link}")
        time.sleep(0.5)
    except Exception:
        continue

# Save results
df = pd.DataFrame(exploit_data).drop_duplicates()
df.to_csv("data_raw/contracts/exploit_metadata.csv", index=False)
print(f"\nâœ… Saved exploit metadata: {len(df)} matches found and saved to data_raw/contracts/exploit_metadata.csv")