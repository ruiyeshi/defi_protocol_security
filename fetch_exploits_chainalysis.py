import requests
import pandas as pd
from bs4 import BeautifulSoup

output_path = "data_raw/contracts/exploit_metadata_chainalysis.csv"

# --- Step 1: Fetch Chainalysis blog ‚ÄúCrypto Crime Reports‚Äù index ---
print("üåê Fetching Chainalysis exploit summaries...")
base_url = "https://blog.chainalysis.com/reports"
r = requests.get(base_url, headers={"User-Agent": "Mozilla/5.0"})
r.raise_for_status()
soup = BeautifulSoup(r.text, "html.parser")

# --- Step 2: Extract report URLs containing 'crime' or 'hacks' ---
links = [a["href"] for a in soup.select("a") if "crime" in a.get("href", "") or "hack" in a.get("href", "")]
links = list(set(links))

exploits = []
for link in links:
    full_url = link if link.startswith("http") else "https://blog.chainalysis.com" + link
    try:
        res = requests.get(full_url, headers={"User-Agent": "Mozilla/5.0"})
        page = BeautifulSoup(res.text, "html.parser")
        text = page.get_text(" ").lower()

        # Extract keywords heuristically
        if "defi" in text and ("exploit" in text or "hack" in text):
            exploits.append({
                "source": "Chainalysis",
                "url": full_url,
                "summary_excerpt": " ".join(text.split()[:100])
            })
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading {link}: {e}")

# --- Step 3: Save ---
df = pd.DataFrame(exploits)
df.to_csv(output_path, index=False)
print(f"‚úÖ Saved {len(df)} Chainalysis-reported DeFi incidents to {output_path}")