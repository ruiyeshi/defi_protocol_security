import json
import requests
from bs4 import BeautifulSoup
from pathlib import Path
from time import sleep

OUTPUT = Path("data_raw/exploits_raw/exploits_combined.json")
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; DeFiResearchBot/1.0; +https://github.com/ruiyeshi)"}

def scrape_rekt():
    print("üì∞ Scraping Rekt.News ...")
    try:
        base_url = "https://rekt.news"
        resp = requests.get(base_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True) if "/rekt" in a["href"] or "/rugged" in a["href"]]
        results = []
        for link in links[:50]:
            url = link if link.startswith("http") else base_url + link
            art = requests.get(url, headers=HEADERS, timeout=15)
            s = BeautifulSoup(art.text, "html.parser")
            title = s.find("h1").get_text(strip=True) if s.find("h1") else "Untitled"
            date = s.find("time").get_text(strip=True) if s.find("time") else ""
            text = " ".join([p.get_text(strip=True) for p in s.find_all("p")])
            results.append({"source": "Rekt.News", "title": title, "url": url, "date": date, "text": text})
            sleep(1)
        print(f"‚úÖ {len(results)} from Rekt.News")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è Rekt.News failed: {e}")
        return []

def scrape_slowmist():
    print("üß† Scraping SlowMist ...")
    try:
        api = "https://slowmist.io/en/rss.xml"
        resp = requests.get(api, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "xml")
        results = []
        for item in soup.find_all("item"):
            title = item.title.get_text(strip=True)
            link = item.link.get_text(strip=True)
            date = item.pubDate.get_text(strip=True)
            desc = item.description.get_text(strip=True)
            results.append({"source": "SlowMist", "title": title, "url": link, "date": date, "text": desc})
        print(f"‚úÖ {len(results)} from SlowMist")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è SlowMist failed: {e}")
        return []

def scrape_web3rekt():
    print("üíª Scraping Web3REKT ...")
    try:
        base = "https://web3rekt.substack.com/archive"
        resp = requests.get(base, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")
        links = [a["href"] for a in soup.select("a[href*='/p/']")]
        results = []
        for link in links[:30]:
            r = requests.get(link, headers=HEADERS, timeout=15)
            s = BeautifulSoup(r.text, "html.parser")
            title = s.find("h1").get_text(strip=True) if s.find("h1") else "Untitled"
            text = " ".join([p.get_text(strip=True) for p in s.find_all("p")])
            results.append({"source": "Web3REKT", "title": title, "url": link, "date": "", "text": text})
        print(f"‚úÖ {len(results)} from Web3REKT")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è Web3REKT failed: {e}")
        return []

def scrape_defiranger():
    print("üîç Scraping DeFiRanger ...")
    try:
        api = "https://defiranger.io/incidents"
        backup = "https://web.archive.org/web/20240401000000/https://defiranger.io/incidents"
        try:
            resp = requests.get(api, headers=HEADERS, timeout=15)
            data = resp.json()
        except:
            print("‚ö†Ô∏è Primary API failed, using Wayback Machine backup...")
            resp = requests.get(backup, headers=HEADERS, timeout=15)
            data = resp.json()
        results = []
        for x in data.get("incidents", []):
            results.append({
                "source": "DeFiRanger",
                "title": x.get("name", "Untitled"),
                "url": x.get("url"),
                "date": x.get("date"),
                "text": x.get("description", "")
            })
        print(f"‚úÖ {len(results)} from DeFiRanger")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è DeFiRanger failed: {e}")
        return []

def scrape_defi_api():
    print("üåê Scraping De.Fi API ...")
    try:
        url = "https://api.de.fi/incidents"
        resp = requests.get(url, headers=HEADERS, timeout=15)
        data = resp.json()
        results = []
        for x in data.get("data", []):
            results.append({
                "source": "De.Fi",
                "title": x.get("title", "Untitled"),
                "url": x.get("link"),
                "date": x.get("date"),
                "text": x.get("description", "")
            })
        print(f"‚úÖ {len(results)} from De.Fi")
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è De.Fi API failed: {e}")
        return []

def main():
    all_data = []
    for func in [scrape_rekt, scrape_slowmist, scrape_web3rekt, scrape_defiranger, scrape_defi_api]:
        try:
            all_data.extend(func())
        except Exception as e:
            print(f"‚ùå {func.__name__} crashed: {e}")
    print(f"\nüíæ Total collected: {len(all_data)} incidents")
    OUTPUT.parent.mkdir(parents=True, exist_ok=True)
    OUTPUT.write_text(json.dumps(all_data, indent=2, ensure_ascii=False))
    print(f"üìÅ Saved to {OUTPUT}")

if __name__ == "__main__":
    main()