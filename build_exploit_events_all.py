#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build a single, repo-clean exploit events dataset and a protocol-level exploit panel.

Primary goal:
  - exploit_events_all.csv : event-level (many rows)
  - protocol_exploit_panel.csv : protocol-level aggregation (one row per exploited protocol)

Inputs (event-level sources; first existing non-empty “defillama-like” source will be used):
  - data_raw/exploits/exploit_events_defillama.csv                 (preferred if present)
  - data_raw/contracts/defillama_hacks_processed.csv               (you have this)
  - data_raw/exploits_raw/defillama_hacks.csv
  - data_raw/exploit_metadata.csv
  - data_raw/contracts/exploit_metadata_final.csv
  - data_raw/contracts/defi_security_master_with_hacks.csv         (ONLY if it contains event-like rows)

Optional additional event sources:
  - data_raw/exploits/exploit_events_rekt.csv                      (from your rekt miner)
  - data_raw/exploits/exploit_events_slowmist.csv                  (slowmist is unstable; often empty)

Optional JSON caches (no network; will be used if present):
  - data_raw/exploits_raw/rekt_news.json
  - data_raw/exploits_raw/slowmist_news.json
  - data_raw/exploits_raw/exploits_combined_unified.json

Protocol universe / mapping:
  - data_raw/llama_protocols.csv                                   (preferred)
  - data_raw/contracts/defillama_top_protocols.csv                 (fallback/augment)

Optional mapping overrides:
  - data_raw/exploits/manual_name_to_slug.csv                      (protocol_name_raw -> slug)

Protocol-level fallback/enrichment (if your earlier work already computed hacks):
  - data_raw/contracts/defi_security_master_with_hacks.csv         (slug + hack summary cols)

Outputs:
  - data_raw/exploits/exploit_events_all.csv
  - data_raw/exploits/protocol_exploit_panel.csv

Event schema (exploit_events_all.csv):
  protocol_name_raw, source, exploit_date, loss_usd, chain, exploit_type, evidence_url, notes,
  slug_final, in_llama

Panel schema (protocol_exploit_panel.csv):
  slug, num_hacks, hack_loss_usd, first_exploit_date, last_exploit_date, exploit_types

Design:
  - No crashes if optional inputs are missing/empty.
  - No garbage rows: require evidence_url + protocol_name_raw.
"""

from __future__ import annotations

from pathlib import Path
import re
import pandas as pd

BASE = Path("data_raw")
EXP = BASE / "exploits"

LLAMA = BASE / "llama_protocols.csv"
LLAMA_ALT = BASE / "contracts" / "defillama_top_protocols.csv"

OVERRIDES = EXP / "manual_name_to_slug.csv"
SECURITY_MASTER_WITH_HACKS = BASE / "contracts" / "defi_security_master_with_hacks.csv"

# Candidate “defillama-like” event sources
DEFILLAMA_CANDIDATES = [
    EXP / "exploit_events_defillama.csv",
    BASE / "contracts" / "defillama_hacks_processed.csv",
    BASE / "exploits_raw" / "defillama_hacks.csv",
    BASE / "exploit_metadata.csv",
    BASE / "contracts" / "exploit_metadata_final.csv",
    BASE / "contracts" / "defi_security_master_with_hacks.csv",  # only if it’s event-like
]

REKT_EVENTS = EXP / "exploit_events_rekt.csv"
SLOWMIST_EVENTS = EXP / "exploit_events_slowmist.csv"

OUT_EVENTS = EXP / "exploit_events_all.csv"
OUT_PANEL = EXP / "protocol_exploit_panel.csv"


def _ensure_dirs() -> None:
    EXP.mkdir(parents=True, exist_ok=True)


def _norm(s: str) -> str:
    s = str(s or "").strip().lower()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("–", "-").replace("—", "-")
    s = re.sub(r"[^a-z0-9\- _./]", "", s)
    return s


def _parse_dt(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return pd.NaT
    return pd.to_datetime(x, errors="coerce", utc=True)


def _parse_loss_usd(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return None
    if isinstance(x, (int, float)):
        try:
            v = float(x)
            return v if v == v else None
        except Exception:
            return None
    s = str(x).replace(",", "").strip()
    m = re.match(r"\\$?\\s*([0-9]*\\.?[0-9]+)\\s*([kKmMbB])?$", s)
    if m:
        val = float(m.group(1))
        suf = (m.group(2) or "").lower()
        mult = {"k": 1e3, "m": 1e6, "b": 1e9}.get(suf, 1.0)
        return val * mult
    m2 = re.search(r"([0-9]*\\.?[0-9]+)", s)
    if m2:
        try:
            return float(m2.group(1))
        except Exception:
            return None
    return None


def _safe_read_csv(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    try:
        df = pd.read_csv(path)
        return df if len(df) else pd.DataFrame()
    except Exception:
        return pd.DataFrame()


def _safe_read_json(path: Path) -> object:
    if not path.exists():
        return None
    try:
        import json
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def _read_llama_like(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    df = pd.read_csv(path)
    if "slug" not in df.columns:
        return pd.DataFrame()
    for c in ["name", "symbol", "category", "tvl", "chains"]:
        if c not in df.columns:
            df[c] = pd.NA
    return df[["slug", "name", "symbol", "category", "tvl", "chains"]].copy()


def _load_llama() -> pd.DataFrame:
    primary = _read_llama_like(LLAMA)
    alt = _read_llama_like(LLAMA_ALT)

    if primary.empty and alt.empty:
        raise SystemExit(
            f"Missing both protocol universe files: {LLAMA} and {LLAMA_ALT}. "
            "Run your llama fetch script (preferred) or place defillama_top_protocols.csv."
        )

    llama = primary.copy() if not primary.empty else alt.copy()

    # augment with alt if both exist
    if not primary.empty and not alt.empty:
        have = set(primary["slug"].astype(str))
        add = alt[~alt["slug"].astype(str).isin(have)].copy()
        if len(add):
            llama = pd.concat([llama, add], ignore_index=True)

    llama["slug"] = llama["slug"].astype(str)
    llama["name"] = llama["name"].astype(str)
    llama["symbol"] = llama["symbol"].astype(str)

    llama["slug_norm"] = llama["slug"].map(_norm)
    llama["name_norm"] = llama["name"].map(_norm)
    llama["symbol_norm"] = llama["symbol"].map(_norm)
    return llama


def _load_overrides() -> dict[str, str]:
    if not OVERRIDES.exists():
        OVERRIDES.parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame({"protocol_name_raw": [], "slug": []}).to_csv(OVERRIDES, index=False)
        return {}

    ov = _safe_read_csv(OVERRIDES)
    if ov.empty:
        return {}

    cols = {c.lower(): c for c in ov.columns}
    pn = cols.get("protocol_name_raw")
    sl = cols.get("slug")
    if not pn or not sl:
        raise SystemExit(f"Override file must have columns protocol_name_raw, slug: {OVERRIDES}")

    out: dict[str, str] = {}
    for _, r in ov.iterrows():
        k = _norm(r[pn])
        v = _norm(r[sl])
        if k and v:
            out[k] = v
    return out


def _map_name_to_slug(name_raw: str, llama: pd.DataFrame, overrides: dict[str, str]) -> str | None:
    n = _norm(name_raw)
    if not n:
        return None
    if n in overrides:
        return overrides[n]

    # cache indexes
    if not hasattr(_map_name_to_slug, "_idx"):
        slug_idx = dict(zip(llama["slug_norm"], llama["slug"]))
        name_idx = dict(zip(llama["name_norm"], llama["slug"]))
        sym_idx = dict(zip(llama["symbol_norm"], llama["slug"]))
        _map_name_to_slug._idx = (slug_idx, name_idx, sym_idx)  # type: ignore[attr-defined]

    slug_idx, name_idx, sym_idx = _map_name_to_slug._idx  # type: ignore[attr-defined]

    if n in slug_idx:
        return slug_idx[n]
    if n in name_idx:
        return name_idx[n]
    if n in sym_idx:
        return sym_idx[n]

    # cleanup: remove common hack words
    n2 = re.sub(r"\\b(hack|hacked|exploit|incident|rekt)\\b", "", n).strip()
    n2 = re.sub(r"\\s+", " ", n2)
    if n2 in name_idx:
        return name_idx[n2]

    return None


def _standardize_defillama(df: pd.DataFrame) -> pd.DataFrame:
    """
    Best-effort standardization for DefiLlama-like hack tables.

    Accepts columns such as:
      name/protocol/protocol_name_raw
      exploit_date/date/exploit_ts
      loss_usd/amount/loss
      chain/ecosystem
      classification/technique/exploit_type
      source_url/evidence_url/url
    """
    base_cols = [
        "protocol_name_raw",
        "source",
        "exploit_date",
        "loss_usd",
        "chain",
        "exploit_type",
        "evidence_url",
        "notes",
    ]
    if df is None or df.empty:
        return pd.DataFrame(columns=base_cols)

    colmap = {c.lower(): c for c in df.columns}

    pn = colmap.get("protocol_name_raw") or colmap.get("name") or colmap.get("protocol")
    dt = colmap.get("exploit_date") or colmap.get("date") or colmap.get("exploit_ts") or colmap.get("exploit_datetime")
    loss = colmap.get("loss_usd") or colmap.get("amount") or colmap.get("loss")
    chain = colmap.get("chain") or colmap.get("ecosystem") or colmap.get("chain_exploit")
    et = colmap.get("exploit_type") or colmap.get("classification") or colmap.get("technique") or colmap.get("exploit_technique")
    url = colmap.get("source_url") or colmap.get("evidence_url") or colmap.get("url") or colmap.get("link")

    out = pd.DataFrame(columns=base_cols)
    out["protocol_name_raw"] = df[pn].astype(str) if pn else ""
    out["source"] = "defillama"
    out["exploit_date"] = df[dt] if dt else pd.NA
    out["loss_usd"] = df[loss] if loss else pd.NA
    out["chain"] = df[chain] if chain else pd.NA
    out["exploit_type"] = df[et] if et else pd.NA
    out["evidence_url"] = df[url] if url else pd.NA

    # preserve extra context if present
    notes_bits = []
    for extra in ["bridgehack", "targettype", "returnedfunds", "defillamaid", "parentprotocolid"]:
        c = colmap.get(extra)
        if c:
            notes_bits.append(extra + "=" + df[c].astype(str))
    if notes_bits:
        out["notes"] = notes_bits[0]
        for s in notes_bits[1:]:
            out["notes"] = out["notes"].astype(str) + " | " + s.astype(str)
    else:
        out["notes"] = ""

    return out


def _standardize_simple_events_from_df(df: pd.DataFrame, source_name: str) -> pd.DataFrame:
    base_cols = [
        "protocol_name_raw",
        "source",
        "exploit_date",
        "loss_usd",
        "chain",
        "exploit_type",
        "evidence_url",
        "notes",
    ]
    if df is None or df.empty:
        return pd.DataFrame(columns=base_cols)

    colmap = {c.lower(): c for c in df.columns}

    pn = colmap.get("protocol_name_raw") or colmap.get("title") or colmap.get("name") or colmap.get("project")
    dt = colmap.get("exploit_date") or colmap.get("date") or colmap.get("publish_time") or colmap.get("exploit_ts")
    loss = colmap.get("loss_usd") or colmap.get("amount") or colmap.get("loss")
    chain = colmap.get("chain") or colmap.get("ecosystem")
    et = colmap.get("exploit_type") or colmap.get("classification") or colmap.get("category") or colmap.get("attack_method") or colmap.get("technique")
    url = colmap.get("evidence_url") or colmap.get("source_url") or colmap.get("url") or colmap.get("link")

    out = pd.DataFrame(columns=base_cols)
    out["protocol_name_raw"] = df[pn].astype(str) if pn else ""
    out["source"] = source_name
    out["exploit_date"] = df[dt] if dt else pd.NA
    out["loss_usd"] = df[loss] if loss else pd.NA
    out["chain"] = df[chain] if chain else pd.NA
    out["exploit_type"] = df[et] if et else pd.NA
    out["evidence_url"] = df[url] if url else pd.NA
    out["notes"] = ""
    return out


def _standardize_simple_events(path: Path, source_name: str) -> pd.DataFrame:
    df = _safe_read_csv(path)
    return _standardize_simple_events_from_df(df, source_name)


def _standardize_json_records(obj: object, source_name: str) -> pd.DataFrame:
    base_cols = [
        "protocol_name_raw",
        "source",
        "exploit_date",
        "loss_usd",
        "chain",
        "exploit_type",
        "evidence_url",
        "notes",
    ]
    if obj is None:
        return pd.DataFrame(columns=base_cols)

    records = None
    if isinstance(obj, list):
        records = obj
    elif isinstance(obj, dict):
        for k in ("data", "list", "items", "results"):
            v = obj.get(k)
            if isinstance(v, list):
                records = v
                break
        if records is None and all(isinstance(v, dict) for v in obj.values()):
            records = list(obj.values())

    if not records:
        return pd.DataFrame(columns=base_cols)

    df = pd.DataFrame.from_records(records)
    return _standardize_simple_events_from_df(df, source_name)


def _read_first_existing(paths: list[Path]) -> tuple[Path | None, pd.DataFrame]:
    for p in paths:
        df = _safe_read_csv(p)
        if not df.empty:
            return p, df
    return None, pd.DataFrame()


def _load_protocol_panel_fallback() -> pd.DataFrame:
    """
    Optional: load earlier protocol-level hack summary and standardize to panel schema.

    We only use it to *augment* protocol_exploit_panel.csv if event mapping misses slugs.

    Returns standardized columns:
      slug, num_hacks, hack_loss_usd, first_exploit_date, last_exploit_date, exploit_types
    """
    df = _safe_read_csv(SECURITY_MASTER_WITH_HACKS)
    if df.empty:
        return pd.DataFrame()

    cols = {c.lower(): c for c in df.columns}

    slug_c = cols.get("slug") or cols.get("slug_final")
    if not slug_c:
        return pd.DataFrame()

    num_c = cols.get("num_hacks") or cols.get("hack_count") or cols.get("hack_events") or cols.get("num_exploits")
    loss_c = cols.get("hack_loss_usd") or cols.get("total_loss_usd") or cols.get("loss_usd") or cols.get("loss")
    first_c = cols.get("first_exploit_date") or cols.get("first_hack_date") or cols.get("first_incident_date") or cols.get("exploit_date")
    last_c = cols.get("last_exploit_date") or cols.get("last_hack_date") or cols.get("last_incident_date")
    types_c = cols.get("exploit_types") or cols.get("exploit_type") or cols.get("technique")

    out = pd.DataFrame()
    out["slug"] = df[slug_c].astype(str).str.strip()

    if num_c:
        out["num_hacks"] = pd.to_numeric(df[num_c], errors="coerce")
    else:
        exp_c = cols.get("exploited") or cols.get("exploited1") or cols.get("has_hack")
        out["num_hacks"] = pd.to_numeric(df[exp_c], errors="coerce") if exp_c else pd.NA

    out["hack_loss_usd"] = pd.to_numeric(df[loss_c].map(_parse_loss_usd), errors="coerce") if loss_c else pd.NA
    out["first_exploit_date"] = df[first_c].map(_parse_dt) if first_c else pd.NaT
    out["last_exploit_date"] = df[last_c].map(_parse_dt) if last_c else pd.NaT
    out["exploit_types"] = df[types_c].astype(str) if types_c else ""

    out = out[out["slug"].notna() & (out["slug"] != "")]
    out["num_hacks"] = pd.to_numeric(out["num_hacks"], errors="coerce")
    out["hack_loss_usd"] = pd.to_numeric(out["hack_loss_usd"], errors="coerce")

    return out[["slug", "num_hacks", "hack_loss_usd", "first_exploit_date", "last_exploit_date", "exploit_types"]]


def main() -> None:
    _ensure_dirs()

    llama = _load_llama()
    overrides = _load_overrides()

    # --- Load primary defillama-like table ---
    src_defi_path, src_defi_df = _read_first_existing(DEFILLAMA_CANDIDATES)
    defillama = _standardize_defillama(src_defi_df)

    frames = [defillama]

    # --- Optional JSON caches (no network) ---
    rekt_json = _safe_read_json(BASE / "exploits_raw" / "rekt_news.json")
    slowmist_json = _safe_read_json(BASE / "exploits_raw" / "slowmist_news.json")
    unified_json = _safe_read_json(BASE / "exploits_raw" / "exploits_combined_unified.json")

    if rekt_json is not None:
        frames.append(_standardize_json_records(rekt_json, "rekt"))
    if slowmist_json is not None:
        frames.append(_standardize_json_records(slowmist_json, "slowmist"))
    if unified_json is not None:
        frames.append(_standardize_json_records(unified_json, "unified"))

    # --- Optional CSV event sources ---
    if REKT_EVENTS.exists():
        frames.append(_standardize_simple_events(REKT_EVENTS, "rekt"))
    if SLOWMIST_EVENTS.exists():
        frames.append(_standardize_simple_events(SLOWMIST_EVENTS, "slowmist"))

    ev = pd.concat(frames, ignore_index=True)

    # --- Clean ---
    ev["protocol_name_raw"] = ev.get("protocol_name_raw", "").astype(str).str.strip()
    ev["evidence_url"] = ev.get("evidence_url", "").astype(str).str.strip()

    # NO GARBAGE: require url + protocol label
    ev = ev[(ev["evidence_url"].notna()) & (ev["evidence_url"] != "")]
    ev = ev[(ev["protocol_name_raw"].notna()) & (ev["protocol_name_raw"] != "")]

    # parse date + loss
    ev["exploit_date"] = ev.get("exploit_date", pd.NA).map(_parse_dt)
    ev["loss_usd"] = ev.get("loss_usd", pd.NA).map(_parse_loss_usd)

    ev["chain"] = ev.get("chain", "").astype(str).map(_norm).replace({"nan": ""})
    ev["exploit_type"] = ev.get("exploit_type", "").astype(str).map(lambda x: x.strip() if x else "").replace({"nan": ""})

    # --- Map to llama slugs ---
    ev["slug_final"] = [_map_name_to_slug(n, llama, overrides) for n in ev["protocol_name_raw"].tolist()]

    llama_slugs = set(llama["slug"].astype(str))
    ev["in_llama"] = ev["slug_final"].astype(str).isin(llama_slugs).astype(int)
    ev.loc[
        ev["slug_final"].isna()
        | (ev["slug_final"].astype(str).str.strip() == "")
        | (ev["slug_final"] == "None"),
        "in_llama",
    ] = 0

    # --- Dedup by URL ---
    ev["_url_norm"] = ev["evidence_url"].map(_norm)
    ev = ev.drop_duplicates(subset=["_url_norm"], keep="first")

    # --- Write event master ---
    keep_cols = [
        "protocol_name_raw",
        "source",
        "exploit_date",
        "loss_usd",
        "chain",
        "exploit_type",
        "evidence_url",
        "notes",
        "slug_final",
        "in_llama",
    ]
    for c in keep_cols:
        if c not in ev.columns:
            ev[c] = pd.NA
    ev = ev[keep_cols]
    ev.to_csv(OUT_EVENTS, index=False)

    # --- Protocol panel from mapped events ---
    m = ev[(ev["slug_final"].notna()) & (ev["in_llama"] == 1)].copy()

    def _uniq_join(values) -> str:
        vals = [v for v in values if isinstance(v, str) and v.strip()]
        return ";".join(sorted(set(vals)))

    panel = (
        m.groupby("slug_final")
        .agg(
            num_hacks=("evidence_url", "count"),
            hack_loss_usd=("loss_usd", lambda s: float(pd.to_numeric(s, errors="coerce").fillna(0).sum())),
            first_exploit_date=("exploit_date", "min"),
            last_exploit_date=("exploit_date", "max"),
            exploit_types=("exploit_type", _uniq_join),
        )
        .reset_index()
        .rename(columns={"slug_final": "slug"})
    )

    # --- Enrich panel with protocol-level fallback if present ---
    fb = _load_protocol_panel_fallback()
    if not fb.empty:
        panel["slug"] = panel["slug"].astype(str)
        fb["slug"] = fb["slug"].astype(str)

        merged = panel.merge(fb, on="slug", how="outer", suffixes=("", "__fb"))

        merged["num_hacks"] = merged["num_hacks"].fillna(merged.get("num_hacks__fb"))
        merged["hack_loss_usd"] = merged["hack_loss_usd"].fillna(merged.get("hack_loss_usd__fb"))

        if "first_exploit_date__fb" in merged.columns:
            merged["first_exploit_date"] = pd.to_datetime(merged["first_exploit_date"], errors="coerce", utc=True)
            merged["first_exploit_date__fb"] = pd.to_datetime(merged["first_exploit_date__fb"], errors="coerce", utc=True)
            merged["first_exploit_date"] = merged[["first_exploit_date", "first_exploit_date__fb"]].min(axis=1)

        if "last_exploit_date__fb" in merged.columns:
            merged["last_exploit_date"] = pd.to_datetime(merged["last_exploit_date"], errors="coerce", utc=True)
            merged["last_exploit_date__fb"] = pd.to_datetime(merged["last_exploit_date__fb"], errors="coerce", utc=True)
            merged["last_exploit_date"] = merged[["last_exploit_date", "last_exploit_date__fb"]].max(axis=1)

        if "exploit_types__fb" in mergedжаў.columns:
            def _union_types(a, b):
                aa = [x.strip() for x in str(a or "").split(";") if x.strip() and str(a) != "nan"]
                bb = [x.strip() for x in str(b or "").split(";") if x.strip() and str(b) != "nan"]
                return ";".join(sorted(set(aa + bb)))
            merged["exploit_types"] = [
                _union_types(a, b) for a, b in zip(merged["exploit_types"], merged.get("exploit_types__fb"))
            ]

        drop_cols = [c for c in merged.columns if c.endswith("__fb")]
        merged = merged.drop(columns=drop_cols, errors="ignore")

        panel = merged

        panel["num_hacks"] = pd.to_numeric(panel["num_hacks"], errors="coerce").fillna(0).astype(int)
        panel["hack_loss_usd"] = pd.to_numeric(panel["hack_loss_usd"], errors="coerce").fillna(0.0)

    panel = panel[["slug", "num_hacks", "hack_loss_usd", "first_exploit_date", "last_exploit_date", "exploit_types"]]
    panel.to_csv(OUT_PANEL, index=False)

    # --- Stats ---
    print(f"✅ Wrote: {OUT_EVENTS} | rows={len(ev)}")
    if src_defi_path:
        print(f"   defillama source: {src_defi_path}")
    print(f"✅ Wrote: {OUT_PANEL} | rows={len(panel)}")
    print(f"Mapped-to-llama rate: {(ev['in_llama']==1).mean():.4f}")
    print(f"Unique slugs mapped: {m['slug_final'].nunique()}")

    unm = ev[ev["in_llama"] == 0]
    if not unm.empty:
        top = unm["protocol_name_raw"].astype(str).map(_norm).value_counts().head(20)
        print("Top unmatched names:")
        print(top.to_string())
        print(f"Manual mapping file: {OVERRIDES}")


if __name__ == "__main__":
    main()